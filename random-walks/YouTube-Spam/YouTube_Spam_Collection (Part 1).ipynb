{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#YouTube Spam Collection Data Set (Part 1)\n",
    "\n",
    "Source: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)\n",
    "\n",
    "Original Source: [YouTube Spam Collection v. 1](http://dcomp.sor.ufscar.br/talmeida/youtubespamcollection/)\n",
    "\n",
    "> Alberto, T.C., Lochter J.V., Almeida, T.A. __Filtragem Automática de Spam nos Comentários do YouTube.__ Anais do XII Encontro Nacional de Inteligência Artificial e Computacional (ENIAC'15), Natal, RN, Brazil, 2015. ([preprint](http://dcomp.sor.ufscar.br/talmeida/papers/TCA_ENIAC15.pdf))\n",
    "\n",
    "> Alberto, T.C., Lochter J.V., Almeida, T.A. __TubeSpam: Comment Spam Filtering on YouTube.__ Proceedings of the 14th IEEE International Conference on Machine Learning and Applications (ICMLA'15), 1-6, Miami, FL, USA, December, 2015. ([preprint](http://dcomp.sor.ufscar.br/talmeida/papers/TCA_ICMLA15.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Contents\n",
    "* [1 Data Set Description](#section1)\n",
    "* [2 Approach](#section2)\n",
    "* [3 Solution](#section3)\n",
    "  * [3a Import modules](#section3a)\n",
    "  * [3b Read the data set](#section3b)\n",
    "  * [3c Data cleanup](#section3c)\n",
    "  * [3d Split the data](#section3d)\n",
    "  * [3e Transform the data](#section3e)\n",
    "  * [3f Build the model](#section3f)\n",
    "  * [3g Run predictions](#section3g)\n",
    "  * [3h Score the prediction](#section3h)\n",
    "  * [3i Analyze the results](#section3i)\n",
    "  * [3j Other learnings](#section3j)\n",
    "* [4 Summary](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "##1. Data Set Description\n",
    "\n",
    "From the description accompanying the data set, \"the samples were extracted from the comments section of five videos that were among the 10 most viewed on YouTube during the collection period.\"\n",
    "\n",
    "The data is available in five distinct data sets, and the data is classified as 1 for \"spam\" and 0 for \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "##2. Approach\n",
    "Since the data set is split across five data sets, we will take two passes at the data. \n",
    "\n",
    "In this notebook, we will only consider the Psy data set. We will use this as a way to wrap our hands around the problem. We will not do any model tuning in this round.\n",
    "\n",
    "Our second pass will involve merging all five data sets and then running the classification on the combined data set. In this round, we will also tune the model and the vectorizer to eke out some improvements. The notebook for this can be accessed [here](https://github.com/vkpedia/databuff/blob/master/random-walks/YouTube-Spam/YouTube_Spam_Collection%20%28Part%202%29.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "##3. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3a'></a>\n",
    "###Import initial set of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3b'></a>\n",
    "###Read in the data from the first CSV alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set; print the first few rows\n",
    "psy = pd.read_csv('data\\\\Youtube01-Psy.csv')\n",
    "psy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3c'></a>\n",
    "###Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 5 columns):\n",
      "COMMENT_ID    350 non-null object\n",
      "AUTHOR        350 non-null object\n",
      "DATE          350 non-null object\n",
      "CONTENT       350 non-null object\n",
      "CLASS         350 non-null int64\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "psy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    175\n",
       "0    175\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like there are no missing values. Let's proceed.\n",
    "# Of the five columns, the only relevant columns for spam/ham classification are the CONTENT and CLASS columns.\n",
    "# We will use just these two columns. But first, let's check the distribution of spam and ham \n",
    "\n",
    "psy.CLASS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There is an equal distribution. Given that this is a small data set, this is probably good, \n",
    "# because the algorithm has enough items it can learn from\n",
    "# Now, let us set up our X and y\n",
    "X = psy.CONTENT\n",
    "y = psy.CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3d'></a>\n",
    "###Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us now split the data set into train and test sets\n",
    "# We will use an 80/20 split\n",
    "test_size = 0.2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3e'></a>\n",
    "###Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a vectorizer, and create a Document-Term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<280x1221 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3560 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the layout of the Document-Term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3f'></a>\n",
    "###Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will build a Naive Bayes model for the Psy data set\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Fit the training data (the DTM, to be precise)\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3g'></a>\n",
    "###Run the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the test data to a DTM and predict\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "y_pred_nb = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3h'></a>\n",
    "###Score the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97142857142857142"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check the accuracy score\n",
    "# It needs to better than 50%, which was the baseline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "accuracy_score(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26,  1],\n",
       "       [ 1, 42]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracy score was 97%, which is indeed good. \n",
    "# Let us check the confusion matrix to get a sense of the prediction distribution\n",
    "confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208    P E A C E  &amp;  L O V E  ! !﻿\n",
       "Name: CONTENT, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model predicted 68 out of 70 instances correctly\n",
    "# We had one false positive and one false negative\n",
    "\n",
    "# What was the false positive comment? (That is, ham marked as spam)\n",
    "X_test[y_pred_nb > y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "288    if i reach 100 subscribers i will go round in ...\n",
       "Name: CONTENT, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what was the false negative comment? (That is, a spam comment that went undetected)\n",
    "X_test[y_pred_nb < y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both these cases are interesting. If one were to hazard a guess, the reason for the false positive could have been that the algorithm was unable to make out the words \"PEACE\" and \"LOVE\" because of the way they appeared in the comment.\n",
    "\n",
    "The false negative comment could have been due to the way the comment was worded. We will check out the top spam and ham words, but before that, let us check the area under the ROC curve, which I anticipate would be pretty high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982773471145564"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, nb.predict_proba(X_test_dtm)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is! Now, let us check out the top spam and ham keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3i'></a>\n",
    "###Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the tokens from the vectorization process earlier\n",
    "X_train_tokens = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.,   1.,   0., ...,   1.,   1.,   1.],\n",
       "       [  0.,   0.,   1., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the feature count from the Naive Bayes model\n",
    "# The first line represents class 0, i.e. ham, and the second line represents spam\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.,   1.,   0., ...,   1.,   1.,   1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_token_count = nb.feature_count_[0, :]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_token_count = nb.feature_count_[1, :]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getting</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giveaways</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_thqbeum69aqup1ih</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham  spam\n",
       "token                       \n",
       "appreciate         0.0   3.0\n",
       "getting            2.0   2.0\n",
       "drugs              0.0   1.0\n",
       "giveaways          0.0   1.0\n",
       "chinese            2.0   0.0\n",
       "got                4.0   3.0\n",
       "who                3.0   2.0\n",
       "stay               0.0   1.0\n",
       "number             1.0   0.0\n",
       "_thqbeum69aqup1ih  0.0   1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will create a data frame of all tokens, and their corresponding ham and spam scores\n",
    "tokens = pd.DataFrame({'token': X_train_tokens, 'ham': ham_token_count, 'spam': \n",
    "    spam_token_count}).set_index('token')\n",
    "\n",
    "# Here are 10 random values drawn from the data frame\n",
    "# Note that these are absolute occurrences\n",
    "tokens.sample(10, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 148.,  132.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the word \"getting\" for example. Was it a ham word or a spam word?\n",
    "# If we concluded that it was equal-chance spam and ham, that would be incorrect\n",
    "# That is because we do not know the proportion of words marked as ham and spam\n",
    "# In this step, we shall standardize this to come up with a truly comparable score\n",
    "class_count = nb.class_count_\n",
    "\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>appreciate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>getting</th>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drugs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>giveaways</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chinese</th>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.015152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stay</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_thqbeum69aqup1ih</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ham      spam\n",
       "token                                \n",
       "appreciate         0.000000  0.022727\n",
       "getting            0.013514  0.015152\n",
       "drugs              0.000000  0.007576\n",
       "giveaways          0.000000  0.007576\n",
       "chinese            0.013514  0.000000\n",
       "got                0.027027  0.022727\n",
       "who                0.020270  0.015152\n",
       "stay               0.000000  0.007576\n",
       "number             0.006757  0.000000\n",
       "_thqbeum69aqup1ih  0.000000  0.007576"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.ham /= class_count[0]\n",
    "tokens.spam /= class_count[1]\n",
    "\n",
    "tokens.sample(10, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This gives us a sense for the relative spamminess and hamminess of a word\n",
    "# In this case, 'getting' is slightly more spammy\n",
    "\n",
    "# Now that we know this, we can compute which words tends to be the most spammy\n",
    "# And which tend to be the least spammy\n",
    "# Please note that since some words have a ham ratio of 0\n",
    "# We can avoid dividing by 0 by adding the same small value to both the numerator and denominator\n",
    "words_with_spam_score = ((tokens.spam + 1e-6)/(tokens.ham + 1e-6)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3j'></a>\n",
    "###Learnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['www', 'subscribe', 'channel', 'com', 'image2you', 'ru', '48051',\n",
       "       'https', 'videos', 'guys', 'do', 'thanks', 'facebook', 'our', 'amp',\n",
       "       'follow', 'co', 'sub', 'pl', 'dolacz', 'v3veygin', 'ermail', 'free',\n",
       "       'tsu', 'll', 'twitter', 'plz', 'gt', 'org', 'subscribers', 'friend',\n",
       "       'share', 'give', 'twitch', 'hi', 'minecraft', 'la', 'chance', 'play',\n",
       "       'clothes', 'remix', 'tv', 'news', 'gaming', 'network', 'cards', 'gift',\n",
       "       'subs', 'need', 'earn'],\n",
       "      dtype='object', name='token')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 50 most spam words\n",
    "words_with_spam_score[:50].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['actually', '5million', 'dick', 'section', 'dislike', 'salt',\n",
       "       '9bzkp7q19f0', 'pray', 'population', 'dislikes', 'planet', 'piece',\n",
       "       'person', 'ching', 'other', 'holy', 'wanted', 'gets', 'guy', '강남스타일',\n",
       "       'fucking', 'fuck', 'every', 'korea', 'mean', 'saying', 'justin', 'been',\n",
       "       'understand', 'baby', 'likes', '2billion', 'ago', 'checking', 'hits',\n",
       "       'popular', 'wow', 'million', 'gangnam', 'viewed', 'years', 'over',\n",
       "       'shit', 'came', 'still', 'most', 'he', '000', 'billion', 'views'],\n",
       "      dtype='object', name='token')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 50 most ham words\n",
    "words_with_spam_score[-50:].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "##4. Summary\n",
    "\n",
    "In this notebook, we built a machine learning model based on comments for a YouTube video. The training data set had a total of 350 observations, half of which was spam. We used a Naive Bayes classifier as our algorithm, and trained it on 80% of the data set. The model resulted in an accuracy score of 97% on the test data. We used the model to derive the top 50 most spammy and least spammy keywords.\n",
    "\n",
    "In the following notebook, we will extend this to the entire data set, consisting of 1,956 observations spread across five videos. This notebook can be accessed [here]()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
