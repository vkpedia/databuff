{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#YouTube Spam Collection Data Set (Part 2)\n",
    "\n",
    "Source: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)\n",
    "\n",
    "Original Source: [YouTube Spam Collection v. 1](http://dcomp.sor.ufscar.br/talmeida/youtubespamcollection/)\n",
    "\n",
    "> Alberto, T.C., Lochter J.V., Almeida, T.A. __Filtragem Automática de Spam nos Comentários do YouTube.__ Anais do XII Encontro Nacional de Inteligência Artificial e Computacional (ENIAC'15), Natal, RN, Brazil, 2015. ([preprint](http://dcomp.sor.ufscar.br/talmeida/papers/TCA_ENIAC15.pdf))\n",
    "\n",
    "> Alberto, T.C., Lochter J.V., Almeida, T.A. __TubeSpam: Comment Spam Filtering on YouTube.__ Proceedings of the 14th IEEE International Conference on Machine Learning and Applications (ICMLA'15), 1-6, Miami, FL, USA, December, 2015. ([preprint](http://dcomp.sor.ufscar.br/talmeida/papers/TCA_ICMLA15.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Contents\n",
    "* [1 Data Set Description](#section1)\n",
    "* [2 Approach](#section2)\n",
    "* [3 Solution](#section3)\n",
    "  * [3a Import modules](#section3a)\n",
    "  * [3b Read the data set](#section3b)\n",
    "  * [3c Data cleanup](#section3c)\n",
    "  * [3d Split the data](#section3d)\n",
    "  * [3e Transform the data](#section3e)\n",
    "  * [3f Build the model](#section3f)\n",
    "  * [3g Run predictions](#section3g)\n",
    "  * [3h Score the prediction](#section3h)\n",
    "* [4 Summary](#section4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "##1. Data Set Description\n",
    "\n",
    "From the description accompanying the data set, \"the samples were extracted from the comments section of five videos that were among the 10 most viewed on YouTube during the collection period.\"\n",
    "\n",
    "The data is available in five distinct data sets, and the data is classified as 1 for \"spam\" and 0 for \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "##2. Approach\n",
    "Since the data set is split across five data sets, we will take two passes at the data. This is the second pass.\n",
    "\n",
    "In the (optional) first pass, we considered only the Psy data set, as a way to wrap our hands around the problem. The notebook for this can be accessed [here](https://github.com/vkpedia/databuff/blob/master/random-walks/YouTube-Spam/YouTube_Spam_Collection%20%28Part%201%29.ipynb).\n",
    "\n",
    "Our second pass will involve merging all five data sets and then running the classification on the combined data set. In this round, we will also tune the model and the vectorizer to eke out some improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "##3. Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3a'></a>\n",
    "###Import initial set of modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3b'></a>\n",
    "###Read in the data from the first CSV alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID            AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU         Julius NM   \n",
       "1  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "2  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "3          z13jhp0bxqncu512g22wvzkasxmvvzjaz04   ElNino Melendez   \n",
       "4          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw            GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...   \n",
       "2  2013-11-08T17:34:21             just for test I have to say murdev.com   \n",
       "3  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "4  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data set; print the first few rows\n",
    "files = ['data\\\\Youtube01-Psy.csv', 'data\\\\Youtube02-KatyPerry.csv', 'data\\\\Youtube03-LMFAO.csv', \n",
    "         'data\\\\Youtube04-Eminem.csv', 'data\\\\Youtube05-Shakira.csv']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    df = df.append(pd.read_csv(file))\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3c'></a>\n",
    "###Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1956 entries, 0 to 369\n",
      "Data columns (total 5 columns):\n",
      "COMMENT_ID    1956 non-null object\n",
      "AUTHOR        1956 non-null object\n",
      "DATE          1711 non-null object\n",
      "CONTENT       1956 non-null object\n",
      "CLASS         1956 non-null int64\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 91.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1005\n",
       "0     951\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looks like there are missing values in the DATE column, but it is not a column of interest. Let's proceed.\n",
    "# Of the five columns, the only relevant columns for spam/ham classification are the CONTENT and CLASS columns.\n",
    "# We will use just these two columns. But first, let's check the distribution of spam and ham \n",
    "\n",
    "df.CLASS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There is an almost equal distribution. Given that this is a small data set, this is probably good, \n",
    "# because the algorithm has enough items it can learn from\n",
    "# Now, let us set up our X and y\n",
    "X = df.CONTENT\n",
    "y = df.CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3d'></a>\n",
    "###Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us now split the data set into train and test sets\n",
    "# We will use an 80/20 split\n",
    "test_size = 0.2\n",
    "seed = 42\n",
    "scoring = 'accuracy'\n",
    "num_folds = 10\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = []\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "lr = ('LR', LogisticRegression())\n",
    "knn = ('KNN', KNeighborsClassifier())\n",
    "svc = ('SVC', SVC())\n",
    "nb = ('NB', MultinomialNB())\n",
    "cart = ('CART', DecisionTreeClassifier())\n",
    "\n",
    "models.extend([lr,  knn, svc, nb, cart])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3e'></a>\n",
    "###Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a vectorizer, and create a Document-Term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1564x3810 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20546 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the layout of the Document-Term matrix\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3f'></a>\n",
    "###Build the model\n",
    "\n",
    "In this step, we will build 6 models, and pick the one with the best accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    score = cross_val_score(model, X_train_dtm, y_train, scoring=scoring, cv=kfold)\n",
    "    names.append(name)\n",
    "    results.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, \\\n",
    "    RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "ensembles = []\n",
    "ensemble_names = []\n",
    "ensemble_results = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier()))\n",
    "ensembles.append(('ET', ExtraTreesClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, model in ensembles:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "    score = cross_val_score(model, X_train_dtm, y_train, cv=kfold, scoring=scoring)\n",
    "    ensemble_names.append(name)\n",
    "    ensemble_results.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CART</th>\n",
       "      <td>0.952041</td>\n",
       "      <td>0.011211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.008471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>0.947575</td>\n",
       "      <td>0.008471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB</th>\n",
       "      <td>0.922028</td>\n",
       "      <td>0.022510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.032262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.868333</td>\n",
       "      <td>0.032262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.591458</td>\n",
       "      <td>0.032478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ET</th>\n",
       "      <td>0.591458</td>\n",
       "      <td>0.032478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean       std\n",
       "model                    \n",
       "CART   0.952041  0.011211\n",
       "LR     0.947575  0.008471\n",
       "AB     0.947575  0.008471\n",
       "NB     0.922028  0.022510\n",
       "KNN    0.868333  0.032262\n",
       "RF     0.868333  0.032262\n",
       "SVC    0.591458  0.032478\n",
       "ET     0.591458  0.032478"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_list = []\n",
    "for i, name in enumerate(names):\n",
    "    d = {'model': name, 'mean': results[i].mean(), 'std': results[i].std()}\n",
    "    models_list.append(d)\n",
    "for i, name in enumerate(ensemble_names):\n",
    "    d = {'model': name, 'mean': results[i].mean(), 'std': results[i].std()}\n",
    "    models_list.append(d)\n",
    "    \n",
    "models_df = pd.DataFrame(models_list).set_index('model')\n",
    "models_df.sort_values('mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Model selection\n",
    "\n",
    "Based on accuracy scores, the best algorithm is the Decision Tree Classifier. Logistic Regression and AdaBoost Classifier also performed very well. We will choose Decision Tree as our model, and look to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CART',\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'min_samples_split': 7, 'splitter': 'best'} 0.953964194373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "final_model = DecisionTreeClassifier()\n",
    "\n",
    "criterion_values = ['gini', 'entropy']\n",
    "splitter_values = ['best', 'random']\n",
    "min_samples_split_values = np.arange(2, 11, 1)\n",
    "\n",
    "param_grid = dict(criterion=criterion_values, splitter=splitter_values, \n",
    "                  min_samples_split=min_samples_split_values)\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=final_model, cv=kfold, scoring=scoring, param_grid=param_grid)\n",
    "\n",
    "grid_result = grid.fit(X_train_dtm, y_train)\n",
    "print(grid_result.best_params_, grid_result.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to eke out some improvement in the performance. The Decision Tree Classifier seems to perform best with the min_samples_split set to 7. We will use this for our final model. Note that the default values for 'criterion' and 'splitter' seem to be part of the best performing set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3g'></a>\n",
    "###Run the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=7, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=42, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = DecisionTreeClassifier(min_samples_split=7, random_state=seed)\n",
    "final_model.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test data to a DTM and predict\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "y_pred = final_model.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3h'></a>\n",
    "###Score the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93367346938775508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check the accuracy score\n",
    "# It needs to better than 50%, which was the baseline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164,  12],\n",
       "       [ 14, 202]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracy score was 93.37%, which is lower than we may have anticipated \n",
    "# Let us check the confusion matrix to get a sense of the prediction distribution\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251    2,000,000,000 out of 7,000,000,000 people in t...\n",
       "221                                      I want new song\n",
       "328    I hate videos like these with those poor anima...\n",
       "254    How did THIS Video in all of YouTube get this ...\n",
       "109    8 million likes xD even the subscribers not 8 ...\n",
       "270    The little PSY is suffering Brain Tumor and on...\n",
       "188    OMG I LOVE YOU KATY PARRY YOUR SONGS ROCK!!!!!...\n",
       "49     thumbs up if u checked this video to see hw vi...\n",
       "289               YOUTUBE MONEY !!!!!!!!!!!!!!!!!!!!!!!﻿\n",
       "306            NEW GOAL!   3,000,000!  Let's go for it!﻿\n",
       "369                         Lemme Top Comments Please!!﻿\n",
       "198    Since when has Katy Perry had her own YouTube ...\n",
       "Name: CONTENT, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model predicted 366 out of 392 instances correctly\n",
    "# We had 14 false positives and 12 false negatives\n",
    "\n",
    "# What were the false positive comments? (That is, ham marked as spam)\n",
    "X_test[y_pred > y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8                         Aslamu Lykum... From Pakistan﻿\n",
       "99     http://thepiratebay.se/torrent/6381501/Timothy...\n",
       "288               los invito a subscribirse a mi canal ﻿\n",
       "262             ｈｔｔｐ://ｗｗｗ.ｅｂａｙ.ｃｏｍ/ｕｓｒ/ｓｈｏｅｃｏｌｌｅｃｔｏｒ314\n",
       "379    Ummm... I just hit 1k subscribers. I make Mine...\n",
       "205    I love katy fashions tiger, care to visit my b...\n",
       "88     Finally someone shares the same opinion as me....\n",
       "109                                I&#39;m A SUBSCRIBER﻿\n",
       "324    Hey yall its the real Kevin Hart, shout out to...\n",
       "128    She loves Vena. trojmiasto.pl/Vena-Bus-Taxi-o5...\n",
       "136    http://thepiratebay.se/torrent/10626048/The.Ex...\n",
       "135    http://thepiratebay.se/torrent/10626835/The.Ex...\n",
       "0              +447935454150 lovely girl talk to me xxx﻿\n",
       "195    if you need youtube subscriber mail hermann bu...\n",
       "Name: CONTENT, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what were the false negative comments? (That is, spam comments that went undetected)\n",
    "X_test[y_pred < y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the false negatives seem like they should have been marked as spam, so it is interesting that the model missed these. We may need to tune our vectorizer and/or attempt some other classifiers.\n",
    "\n",
    "Let us check the area under the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94489162457912457"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, final_model.predict_proba(X_test_dtm)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area is around 94.49%. Not bad for a first pass, but again, this could have been higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "##4. Summary\n",
    "\n",
    "In this notebook, we built a machine learning model based on comments for five YouTube videos. The training data set had a total of 1,956 observations, about half of which was spam. We used 80% of the data set for training, and used 10-fold validation to find among eight algorithms the best-performing algorithm based on accuracy score. The Decision Tree algorithm had the best accuracy score, so we selected it as our model. We further tuned the model and were able to improve the accuracy score.\n",
    "\n",
    "The final model resulted in an **accuracy score of 93.37%** on the test data, with an AUC of 94.49%. Some false negatives seem concerning, and we will need to delve further into the model to understand it better and to iron out these inaccuracies.\n",
    "\n",
    "Some potential next steps are tuning the vectorizer to use bi-grams, visualizing and tuning the Decision Tree further, and comparing the performance to other tuned classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
